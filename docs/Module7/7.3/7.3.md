---
title: Leader Election & Consensus
sidebar_label: "7.3 Leader Election & Consensus"
description: Understanding leader election basics and high-level concepts of Raft and Paxos in distributed systems
---

# Leader Election & Consensus

> Goal: Understand how distributed systems agree on a single leader and maintain consistency using consensus algorithms.

---

## 7.3.1 Why Leader Election Is Needed ⭐⭐⭐⭐⭐

In distributed systems:

- Multiple nodes exist
- Some operations require coordination
- One node must act as leader

Examples:
- Primary database node
- Master scheduler
- Cluster coordinator

Without leader:
- Conflicts
- Split-brain
- Inconsistent state

---

## 7.3.2 What Is Leader Election?

Leader election is the process of:

- Selecting one node as leader
- Other nodes become followers
- Leader coordinates operations

```mermaid
graph TD
    Node1
    Node2
    Node3
    Node1 --> Leader
    Node2 --> Follower
    Node3 --> Follower
````

---

## 7.3.3 What Happens If Leader Fails? ⭐⭐⭐⭐⭐

System must:

1. Detect leader failure
2. Elect new leader
3. Resume operations

Fast recovery is critical for availability.

---

## 7.3.4 What Is Consensus? ⭐⭐⭐⭐⭐

Consensus is the process by which:

> Multiple distributed nodes agree on a single value.

Examples:

* Who is leader?
* What is the next log entry?
* What is the current state?

Consensus ensures:

* All nodes see same order of events
* No conflicting decisions

---

## 7.3.5 Why Consensus Is Hard

Challenges:

* Network partitions
* Message delays
* Node crashes
* Clock skew

There is no global clock.
Messages may be lost or delayed.

---

# 7.3.6 Paxos (High-Level Only) ⭐⭐⭐⭐

Paxos is a consensus algorithm that:

* Ensures safety (no two nodes decide different values)
* Works despite failures
* Very complex to understand and implement

Key idea:

* Majority agreement required

If majority agrees → value accepted.

---

# 7.3.7 Raft (High-Level Only) ⭐⭐⭐⭐⭐

Raft is a consensus algorithm designed to be easier to understand than Paxos.

Core ideas:

1. Leader election
2. Log replication
3. Safety

---

## Raft States

Each node can be:

* Leader
* Follower
* Candidate

```mermaid
graph TD
    Follower --> Candidate
    Candidate --> Leader
    Leader --> Follower
```

---

## How Raft Elects Leader

1. Followers wait for heartbeat.
2. If no heartbeat → become candidate.
3. Candidate requests votes.
4. Majority vote → becomes leader.

---

## Why Majority Matters

If cluster has 5 nodes:

* Need 3 for majority.
* Ensures no split-brain.

> Majority agreement guarantees safety.

---

## 7.3.8 Split-Brain Problem ⭐⭐⭐⭐

Occurs when:

* Network partition splits cluster
* Two leaders are elected
* Conflicting writes happen

Consensus prevents split-brain by requiring majority.

---

## 7.3.9 Real-World Examples

Systems using consensus:

* Zookeeper
* etcd
* Kubernetes control plane
* Distributed databases

Used for:

* Leader election
* Configuration storage
* Distributed locking
* Metadata coordination

---

## 7.3.10 Leader Election in Payment System

Example:

* Only one node processes settlement batch.
* If leader crashes:

  * New leader elected.
  * Settlement continues.

Prevents:

* Duplicate processing
* Inconsistent state

---

## 7.3.11 Common Interview Mistakes ❌

❌ Ignoring network partitions
❌ Forgetting majority requirement
❌ Confusing replication with consensus
❌ Over-explaining Paxos details

✅ Correct thinking:

* Consensus requires majority
* Leader election enables coordination
* Raft easier to explain than Paxos
* Focus on high-level understanding

---

## Key Takeaways ⭐⭐⭐⭐⭐

* Leader election selects coordinator node
* Consensus ensures agreement across nodes
* Majority vote ensures safety
* Raft simplifies consensus
* Split-brain must be avoided

---

## Interview-Ready One-Liners ⭐

* “Consensus ensures nodes agree on a single value.”
* “Majority prevents split-brain.”
* “Raft is easier to understand than Paxos.”
* “Leader election enables coordinated writes.”
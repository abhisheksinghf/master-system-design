---
title: Latency vs Throughput
sidebar_label: "1.3 Latency vs Throughput"
description: Understanding latency and throughput trade-offs in distributed systems.
---

# MODULE 1.3: Latency vs Throughput â­â­â­â­

> Goal: Understand the difference between latency and throughput, why systems optimize for one over the other, and how design choices affect both.

---

## 1.3.1 Definitions (Start Here)

### Latency
- **Time taken to process a single request**
- Measured in milliseconds (ms)

Example:
```

Request â†’ Response = 120 ms

```

---

### Throughput
- **Number of requests processed per unit time**
- Measured in RPS / QPS (requests per second)

Example:
```

System handles 5,000 requests/second

````

---

## 1.3.2 Simple Analogy (Interview-Friendly)

### Toll Booth Example ğŸš—

- **Latency** â†’ Time for one car to pass the booth
- **Throughput** â†’ Cars passing per minute

```text
Low latency â‰  High throughput
High throughput â‰  Low latency
````

This analogy works GREAT in interviews.

---

## 1.3.3 Visual Difference

```mermaid
graph LR
    A[Client] -->|Latency| B[Server]
```

```mermaid
graph TD
    R1[Req 1]
    R2[Req 2]
    R3[Req 3]
    R4[Req 4]

    R1 --> S[Server]
    R2 --> S
    R3 --> S
    R4 --> S
```

* First diagram â†’ **Latency**
* Second diagram â†’ **Throughput**

---

## 1.3.4 Why This Distinction Matters in System Design

Different systems optimize for different goals.

| System Type       | Priority          |
| ----------------- | ----------------- |
| Payment systems   | Low latency       |
| Trading systems   | Ultra-low latency |
| Analytics systems | High throughput   |
| Logging systems   | High throughput   |
| Video streaming   | Balanced          |

---

## 1.3.5 Low Latency Systems â­â­â­â­

### Characteristics

* Fast response required
* User-facing
* Often synchronous

### Examples

* Payment authorization
* Login APIs
* Search suggestions

### Design Choices

* In-memory caching
* Fewer network hops
* Strong consistency (often)

```mermaid
graph LR
    Client --> Cache --> Service --> DB
```

> Cache reduces latency by avoiding DB calls.

---

## 1.3.6 High Throughput Systems â­â­â­â­

### Characteristics

* Process massive data
* Latency less critical
* Often asynchronous

### Examples

* Log ingestion
* Analytics pipelines
* Event processing

### Design Choices

* Message queues
* Batch processing
* Eventual consistency

```mermaid
graph LR
    Producer --> Queue --> Consumer
```

> Queue absorbs traffic spikes and increases throughput.

---

## 1.3.7 Can a System Optimize for Both?

### Short Answer

**Rarely, and at a cost.**

### Reality

* Optimizing latency often reduces throughput
* Optimizing throughput often increases latency

This is a **classic system design trade-off**.

---

## 1.3.8 Design Choices That Affect Latency

| Design Choice     | Impact           |
| ----------------- | ---------------- |
| Network hops      | Increase latency |
| Synchronous calls | Increase latency |
| Disk I/O          | High latency     |
| Caching           | Reduces latency  |
| Serialization     | Adds latency     |

---

## 1.3.9 Design Choices That Affect Throughput

| Design Choice           | Impact                |
| ----------------------- | --------------------- |
| Asynchronous processing | Increases throughput  |
| Batching                | Increases throughput  |
| Parallelism             | Increases throughput  |
| Backpressure            | Stabilizes throughput |

---

## 1.3.10 Tail Latency (IMPORTANT INTERVIEW TOPIC) â­â­â­â­

### What is Tail Latency?

Latency experienced by the **slowest requests** (p95, p99).

Example:

```
Average latency = 100 ms
p99 latency = 2 seconds
```

### Why It Matters

* Users remember slow requests
* SLAs often defined on p95/p99

### Interview Line â­

> Tail latency matters more than average latency.

---

## 1.3.11 Latency vs Throughput in Microservices

```mermaid
graph LR
    Client --> S1 --> S2 --> S3 --> DB
```

Each hop:

* Adds latency
* Reduces reliability

### System Design Insight â­

> Microservices increase throughput scalability but can increase latency.

---

## 1.3.12 Real Example: Payment vs Analytics

### Payment System

* User waiting
* Must respond fast
* Strong consistency

ğŸ‘‰ Optimize for **latency**

---

### Analytics System

* Batch processing
* User not waiting
* Eventual consistency acceptable

ğŸ‘‰ Optimize for **throughput**

---

## 1.3.13 Common Interview Mistakes âŒ

âŒ Saying â€œhigh throughput means low latencyâ€
âŒ Ignoring tail latency
âŒ Not relating to use cases

âœ… Always ask:

> â€œIs this system latency-sensitive or throughput-oriented?â€

---

## Key Takeaways â­â­â­â­â­

* Latency = time per request
* Throughput = requests per second
* You usually optimize for one
* Tail latency matters most
* System requirements decide priorities

---

## Interview-Ready One-Liners â­

* â€œLatency and throughput are different metrics.â€
* â€œOptimizing one often hurts the other.â€
* â€œTail latency matters more than average.â€
* â€œThis system is throughput-oriented, not latency-sensitive.â€

---

## References & Deep-Dive Resources

### Articles

* [https://www.cloudflare.com/learning/performance/glossary/what-is-latency/](https://www.cloudflare.com/learning/performance/glossary/what-is-latency/)
* [https://aws.amazon.com/what-is/latency/](https://aws.amazon.com/what-is/latency/)
* [https://www.geeksforgeeks.org/difference-between-throughput-and-latency/](https://www.geeksforgeeks.org/difference-between-throughput-and-latency/)

### Videos

* [https://www.youtube.com/watch?v=Z1U3hZx7F7I](https://www.youtube.com/watch?v=Z1U3hZx7F7I) (Latency vs Throughput)
* [https://www.youtube.com/watch?v=K2p1N4h6f7k](https://www.youtube.com/watch?v=K2p1N4h6f7k) (Tail Latency Explained)

### Books

* *Designing Data-Intensive Applications*
* *Site Reliability Engineering* â€“ Google

---

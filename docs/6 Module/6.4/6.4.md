---
title: Rate Limiting
sidebar_label: "6.4 Rate Limiting"
description: Understanding why rate limiting is required and how token bucket and leaky bucket algorithms work
---

# Rate Limiting

> Goal: Understand why rate limiting is required in distributed systems and how common algorithms like Token Bucket and Leaky Bucket work.

---

## 6.4.1 Why Rate Limiting Is Required ⭐⭐⭐⭐⭐

Without rate limiting:

- A single user can overload the system
- Bots can abuse APIs
- Traffic spikes can crash services
- Downstream systems may be overwhelmed

> Rate limiting protects system stability and fairness.

---

## 6.4.2 What Is Rate Limiting?

Rate limiting controls:

- How many requests a client can make
- Within a specific time window

Example:
```

100 requests per minute per user

````

If limit exceeded:
- Return HTTP 429 (Too Many Requests)

---

## 6.4.3 Where Is Rate Limiting Applied?

- API Gateway
- Load balancer
- Reverse proxy
- Application layer

Most common: API Gateway layer.

---

# 6.4.4 Token Bucket Algorithm ⭐⭐⭐⭐⭐

## How It Works

- Bucket holds tokens.
- Tokens are added at fixed rate.
- Each request consumes one token.
- If no tokens → request rejected.

```mermaid
graph TD
    TokenGenerator --> Bucket
    Request -->|Consume Token| Bucket
    Bucket -->|If token available| Process
    Bucket -->|If empty| Reject
````

---

## Key Properties

* Allows bursts
* Smooth average rate
* Widely used

Example:

* 10 tokens added per second
* Bucket capacity = 100
* Short burst allowed up to 100 requests

---

# 6.4.5 Leaky Bucket Algorithm ⭐⭐⭐⭐

## How It Works

* Requests enter bucket (queue)
* Processed at constant rate
* Excess requests dropped

```mermaid
graph TD
    Requests --> BucketQueue --> ConstantOutputRate
```

---

## Key Properties

* Smooth output rate
* No bursts
* Good for traffic shaping

---

# 6.4.6 Token Bucket vs Leaky Bucket ⭐⭐⭐⭐⭐

| Feature       | Token Bucket      | Leaky Bucket    |
| ------------- | ----------------- | --------------- |
| Burst Allowed | Yes               | No              |
| Output Rate   | Variable          | Constant        |
| Use Case      | API rate limiting | Traffic shaping |

---

## 6.4.7 Fixed Window & Sliding Window (Conceptual)

Other approaches:

### Fixed Window

* Count requests per time window
* Simple but bursty at boundaries

### Sliding Window

* More accurate
* Reduces burst issues
* More complex

---

## 6.4.8 Distributed Rate Limiting ⭐⭐⭐⭐

In distributed systems:

* Multiple app instances
* Shared counter needed

Solution:

* Centralized store (Redis)
* Atomic increment operations

Challenge:

* Avoid race conditions
* Maintain performance

---

## 6.4.9 Rate Limiting in a Payment System

Use cases:

* Prevent brute-force login
* Prevent payment API abuse
* Protect fraud service
* Prevent bot attacks

Example:

```
5 failed login attempts per minute
```

---

## 6.4.10 Graceful Handling of Rate Limits

When limit exceeded:

* Return HTTP 429
* Include retry-after header
* Log excessive usage

---

## 6.4.11 Common Interview Mistakes ❌

❌ Not explaining algorithms clearly
❌ Ignoring distributed challenges
❌ Forgetting HTTP 429
❌ Confusing rate limiting with throttling

✅ Correct thinking:

* Protect system stability
* Allow controlled bursts
* Use distributed counters carefully

---

## Key Takeaways ⭐⭐⭐⭐⭐

* Rate limiting prevents abuse and overload
* Token bucket allows bursts
* Leaky bucket enforces smooth rate
* Distributed rate limiting requires shared state
* HTTP 429 used for rejection

---

## Interview-Ready One-Liners ⭐

* “Rate limiting protects system stability.”
* “Token bucket allows bursts.”
* “Leaky bucket smooths traffic.”
* “Use Redis for distributed rate limiting.”
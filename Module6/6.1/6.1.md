---
title: Designing for Failures
sidebar_label: "6.1 Designing for Failures"
description: Understanding why failures are inevitable, identifying single points of failure, and designing redundancy for reliable systems
---

# Designing for Failures

> Goal: Understand why failures are inevitable in distributed systems and how to design systems that remain reliable despite failures.

---

## 6.1.1 Why Failures Are Inevitable ⭐⭐⭐⭐⭐

In distributed systems:

- Servers crash
- Networks fail
- Disks corrupt
- Processes hang
- Deployments break
- Traffic spikes unexpectedly

> Failure is not an exception — it is normal.

If your system does not plan for failure,
it will eventually go down.

---

## 6.1.2 Types of Failures

### 1️⃣ Hardware Failures
- Disk crash
- Server power loss
- Memory corruption

### 2️⃣ Network Failures
- Packet loss
- Network partition
- DNS outage

### 3️⃣ Software Failures
- Bugs
- Memory leaks
- Deadlocks

### 4️⃣ Human Errors
- Wrong configuration
- Bad deployments
- Accidental data deletion

---

## 6.1.3 What Is a Single Point of Failure (SPOF)? ⭐⭐⭐⭐⭐

A Single Point of Failure is a component that:

- If it fails,
- Causes the entire system to fail.

Example:

```mermaid
graph TD
    User --> AppServer --> Database
````

If only one DB exists → DB is a SPOF.

---

## 6.1.4 Eliminating Single Points of Failure

Solution: **Redundancy**

```mermaid
graph TD
    User --> LoadBalancer
    LoadBalancer --> App1
    LoadBalancer --> App2
    App1 --> DBReplica
    App2 --> DBReplica
```

Redundancy means:

* Multiple instances
* Backup systems
* Replication

> No component should be irreplaceable.

---

## 6.1.5 Active-Active vs Active-Passive ⭐⭐⭐⭐

### Active-Active

* All nodes handle traffic
* Load distributed
* Higher throughput

### Active-Passive

* One node active
* Backup node waits
* Failover on crash

Trade-off:

* Active-active → complex but scalable
* Active-passive → simpler but slower failover

---

## 6.1.6 Redundancy at Different Levels

### Application Level

* Multiple app servers

### Database Level

* Replication
* Read replicas

### Infrastructure Level

* Multi-AZ deployment
* Multi-region deployment

---

## 6.1.7 Failure Domains ⭐⭐⭐⭐

A failure domain is the boundary within which failures are correlated.

Examples:

* Same server rack
* Same availability zone
* Same region

Best practice:
Distribute replicas across different failure domains.

---

## 6.1.8 Graceful Degradation ⭐⭐⭐⭐⭐

System continues to operate in reduced capacity during failure.

Example:

* Recommendation service down → checkout still works
* Analytics fails → payment still processed

> Critical paths must survive failure of non-critical components.

---

## 6.1.9 Fail Fast Principle

If a service is unhealthy:

* Detect quickly
* Stop routing traffic
* Prevent cascading failure

Avoid:

* Hanging requests
* Long timeouts

---

## 6.1.10 Designing a Resilient Payment System

Critical path:

* Payment authorization
* Balance update

Non-critical:

* Email
* Analytics
* Logging

If email fails:

* Payment should still succeed.

---

## 6.1.11 Common Interview Mistakes ❌

❌ Ignoring SPOF
❌ Assuming hardware never fails
❌ Forgetting network partitions
❌ Designing only for happy path

✅ Correct thinking:

* Assume everything can fail
* Remove SPOFs
* Add redundancy
* Isolate critical path

---

## Key Takeaways ⭐⭐⭐⭐⭐

* Failures are inevitable
* SPOF must be eliminated
* Redundancy improves reliability
* Distribute across failure domains
* Design for graceful degradation

---

## Interview-Ready One-Liners ⭐

* “Failure is the default state in distributed systems.”
* “Remove single points of failure.”
* “Design for graceful degradation.”
* “Redundancy improves reliability.”
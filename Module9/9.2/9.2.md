---
title: Metrics & Monitoring
sidebar_label: "9.2 Metrics & Monitoring"
description: Understanding key system metrics, monitoring strategies, and alerting in distributed systems
---

# Metrics & Monitoring

> Goal: Understand what metrics to track, how monitoring works, and how alerts maintain system reliability.

---

# 9.2.1 Why Metrics Matter ⭐⭐⭐⭐⭐

Logs tell you **what happened**.

Metrics tell you:
- How often it happens
- How severe it is
- Whether the system is healthy

Without metrics:
- You won’t know if latency is increasing
- You won’t know if error rate is rising
- You won’t detect overload early

> If you don’t measure it, you can’t improve it.

---

# 9.2.2 What Is a Metric?

A metric is a numerical measurement collected over time.

Examples:
- CPU usage
- Memory usage
- Request latency
- Error rate
- Requests per second

Metrics are:
- Aggregated
- Time-series data
- Continuously collected

---

# 9.2.3 The Golden Signals ⭐⭐⭐⭐⭐

Popularized by Google SRE.

The 4 Golden Signals:

1️⃣ Latency  
2️⃣ Traffic  
3️⃣ Errors  
4️⃣ Saturation  

---

## 1️⃣ Latency

Time taken to process a request.

Important:
- Average latency
- p95 latency
- p99 latency

Tail latency matters most.

---

## 2️⃣ Traffic

Amount of load on system.

Examples:
- Requests per second (RPS)
- Queries per second (QPS)

---

## 3️⃣ Errors

Rate of failed requests.

Examples:
- 5xx errors
- Timeout errors

---

## 4️⃣ Saturation

Resource utilization level.

Examples:
- CPU usage
- Memory usage
- Thread pool exhaustion
- Queue length

---

# 9.2.4 Percentiles (Very Important) ⭐⭐⭐⭐⭐

Average latency hides spikes.

Example:
- 99 requests = 50ms
- 1 request = 2000ms

Average looks fine.

p99 shows problem.

> Always monitor p95 or p99 latency.

---

# 9.2.5 Monitoring vs Alerting ⭐⭐⭐⭐

Monitoring:
- Collect metrics
- Visualize dashboards

Alerting:
- Notify when thresholds exceeded

Example:
- Error rate > 5%
- p99 latency > 1 second
- CPU > 85%

---

# 9.2.6 SLA, SLO, and SLI ⭐⭐⭐⭐⭐

## SLI (Service Level Indicator)

Actual measured metric.

Example:
- 99.9% request success rate

---

## SLO (Service Level Objective)

Target value for SLI.

Example:
- 99.9% uptime

---

## SLA (Service Level Agreement)

Formal agreement with customer.

May include penalties.

---

# 9.2.7 Monitoring in Microservices ⭐⭐⭐⭐

Each service must monitor:

- Request latency
- Error rate
- Resource usage
- Dependency latency

Also monitor:
- Database health
- Queue backlog
- External API latency

---

# 9.2.8 Example: Payment System Monitoring ⭐⭐⭐⭐⭐

Critical metrics:

- Payment success rate
- Transaction latency
- Fraud service latency
- DB replication lag
- Queue backlog
- Failed transactions

If:
- Error rate spikes → investigate
- Queue grows → add consumers
- CPU high → scale out

---

# 9.2.9 Common Interview Mistakes ❌

❌ Only mentioning CPU usage  
❌ Ignoring tail latency  
❌ Not distinguishing monitoring and alerting  
❌ Ignoring SLAs  

✅ Correct thinking:
- Monitor golden signals
- Focus on p95/p99
- Define SLOs
- Alert intelligently

---

# Key Takeaways ⭐⭐⭐⭐⭐

- Metrics provide system health visibility
- Golden signals are critical
- Monitor percentiles, not just averages
- SLIs, SLOs, SLAs define reliability
- Alerting must be meaningful

---

# Interview-Ready One-Liners ⭐

- “Monitor the four golden signals.”
- “p99 latency matters more than average.”
- “SLI measures; SLO targets; SLA is contract.”
- “Alert on symptoms, not noise.”

---
